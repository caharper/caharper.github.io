import{_ as u,r as a,o as d,c as r,b as o,f as t,d as e,e as n}from"./app-Dw2r7XDh.js";const p="/assets/base_arch-Ds_m-09R.png",m="/assets/signal_burst-BJK146hq.png",f={},g=n('<div class="hint-container info"><p class="hint-container-title">TL;DR</p><p>In my recent journal publication, <em>Automatic Modulation Classification with Deep Neural Networks</em>, we achieve state-of-the-art performance on the large-scale RadioML 2018.01A dataset with a peak accuracy of 98.9% and an overall accuracy of 63.7%. Automatic modulation classification (AMC) is critical for modern communication systems, and poses a significant time series classification challenge. We meticulously analyze and compare various convolutional deep learning architectures, finding the most instrumental network design components for AMC.</p></div><h2 id="publication-information" tabindex="-1"><a class="header-anchor" href="#publication-information" aria-hidden="true">#</a> Publication Information</h2><p><code>Title</code>: Automatic Modulation Classification with Deep Neural Networks</p><p><code>Authors</code>: Clayton Harper, Mitchell A. Thornton, Eric C. Larson</p><p><code>Venue</code>: MPDI Electronics Journal</p><p><code>Publication Date</code>: September 2023</p>',6),v=t("code",null,"Status",-1),_=n(`<p><code>BibTeX Citation</code>:</p><div class="language-bibtex line-numbers-mode" data-ext="bibtex"><pre class="language-bibtex"><code>@article{harper2023automatic,
  title={Automatic Modulation Classification with Deep Neural Networks},
  author={Harper, Clayton A and Thornton, Mitchell A and Larson, Eric C},
  journal={Electronics},
  volume={12},
  number={18},
  pages={3962},
  year={2023},
  publisher={MDPI}
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2),b={class:"hint-container details"},w=t("summary",null,"Paper PDF",-1),y=n('<h2 id="paper-contributions" tabindex="-1"><a class="header-anchor" href="#paper-contributions" aria-hidden="true">#</a> Paper Contributions</h2><ol><li>We achieve state-of-the-art performance on the RadioML 2018.01A dataset with a peak accuracy of 98.9% and an overall accuracy of 63.7%.</li><li>Thorough investigation on multiple design elements (e.g., self-attention, X-Vectors, dilated convolutions, and Squeeze-and-Excitation blocks, and nonlinear activatin placement) and their impacts on AMC performance.</li><li>Increased receptive field was found to be the most influential factor in improving AMC performance.</li></ol><h2 id="paper-purpose" tabindex="-1"><a class="header-anchor" href="#paper-purpose" aria-hidden="true">#</a> Paper Purpose</h2><p>Although I have published other papers on automatic modulation classification (AMC)<sup class="footnote-ref"><a href="#footnote1">[1]</a><a class="footnote-anchor" id="footnote-ref1"></a></sup><sup class="footnote-ref"><a href="#footnote2">[2]</a><a class="footnote-anchor" id="footnote-ref2"></a></sup>, they were shorter conference publications and served as stepping stones to this work. Additionally, many AMC papers focus heavily on single design elements to improve performance (including my previous work). However, there seemed to be a lack of comprehensive analysis on the impacts of different design elements on AMC performance. This is crucial for understanding which factors are most important for AMC design and for guiding future research in this area. Ultimately, the purpose of this work was centered around an exhaustive analysis of various deep learning architectures using the RadioML 2018.01A dataset to accomplish our goal of finding the most influential network design components for AMC.</p><h2 id="methodology" tabindex="-1"><a class="header-anchor" href="#methodology" aria-hidden="true">#</a> Methodology</h2><p>Our template architecture can be seen below where each colored block represents an adjustable component in our ablation study.</p><div class="hint-container note"><p class="hint-container-title">&quot;Ablation&quot; Study</p><p>An ablation study in this context is a method of systematically removing components from a system to understand their individual impacts on the system&#39;s performance. In other words, we try every combination of components and compare their results to see which components are most influential.</p></div><figure><img src="'+p+'" alt="Template architecture to explore." tabindex="0" loading="lazy"><figcaption>Template architecture to explore.</figcaption></figure><p>Squeeze-and-Excitation (SE) blocks<sup class="footnote-ref"><a href="#footnote3">[3]</a><a class="footnote-anchor" id="footnote-ref3"></a></sup> aim to increase the receptive field of convolutional layers by providing global context through statistical aggregations followed by a recalibration of the feature maps. To make this post more succinct, I will not go into the exact details of our SE blocks; however, the main takeaway is that the mean of each channel is taken, followed by a fully connected layer, and then a sigmoid activation. This is then multiplied with the original feature map to recalibrate the activations. We also explore the impacts of X-Vector (mean and variance) embeddings, dilated convolutions, and nonlinear activation placement. We then compare the performance of each architecture combination on the RadioML 2018.01A dataset.</p><h2 id="results" tabindex="-1"><a class="header-anchor" href="#results" aria-hidden="true">#</a> Results</h2><table><caption>Ablation study performance overview.</caption><thead><tr><th>Model Name</th><th>Notes</th><th>SENet</th><th>Dilated Convolutions</th><th>Final Activation</th><th>Attention</th><th># Params</th><th>Avg. Accuracy</th><th>Max Accuracy</th></tr></thead><tbody><tr><td>0000</td><td></td><td>---</td><td>---</td><td>---</td><td>---</td><td>174,000</td><td>62.9%</td><td>98.6%</td></tr><tr><td>0001</td><td></td><td>---</td><td>---</td><td>---</td><td>✔</td><td>221,088</td><td>62.3%</td><td>97.6%</td></tr><tr><td>0010</td><td></td><td>---</td><td>---</td><td>✔</td><td>---</td><td>174,000</td><td>62.8%</td><td>98.6%</td></tr><tr><td>0011</td><td></td><td>---</td><td>---</td><td>✔</td><td>✔</td><td>221,088</td><td>62.3%</td><td>97.5%</td></tr><tr><td>0100</td><td></td><td>---</td><td>✔</td><td>---</td><td>---</td><td>174,000</td><td>63.2%</td><td>98.9%</td></tr><tr><td>0101</td><td></td><td>---</td><td>✔</td><td>---</td><td>✔</td><td>221,088</td><td>63.1%</td><td>97.9%</td></tr><tr><td>0110</td><td></td><td>---</td><td>✔</td><td>✔</td><td>---</td><td>174,000</td><td>63.2%</td><td>98.9%</td></tr><tr><td>0111</td><td></td><td>---</td><td>✔</td><td>✔</td><td>✔</td><td>221,088</td><td>63.0%</td><td>98.0%</td></tr><tr><td>1000</td><td></td><td>✔</td><td>---</td><td>---</td><td>---</td><td>202,880</td><td>62.9%</td><td>98.5%</td></tr><tr><td>1001</td><td></td><td>✔</td><td>---</td><td>---</td><td>✔</td><td>249,968</td><td>62.6%</td><td>98.2%</td></tr><tr><td>1010</td><td></td><td>✔</td><td>---</td><td>✔</td><td>---</td><td>202,880</td><td>62.6%</td><td>98.3%</td></tr><tr><td>1011</td><td></td><td>✔</td><td>---</td><td>✔</td><td>✔</td><td>249,968</td><td>62.8%</td><td>98.1%</td></tr><tr><td>1100</td><td></td><td>✔</td><td>✔</td><td>---</td><td>---</td><td>202,880</td><td>62.8%</td><td>98.2%</td></tr><tr><td>1101</td><td></td><td>✔</td><td>✔</td><td>---</td><td>✔</td><td>249,968</td><td>63.0%</td><td>97.7%</td></tr><tr><td>1110</td><td>Overall best performing model</td><td>✔</td><td>✔</td><td>✔</td><td>---</td><td>202,880</td><td>63.7%</td><td>98.9%</td></tr><tr><td>1111</td><td></td><td>✔</td><td>✔</td><td>✔</td><td>✔</td><td>249,968</td><td>63.0%</td><td>97.8%</td></tr></tbody></table><p>Ultimately, we found that the combination of dilated convolutions, SE blocks, and a final activation layer provided the best performance. This model achieved a peak accuracy of 98.9% and an overall accuracy of 63.7%. Each of these factors increase receptive field, which is likely why they improved AMC performance. Self-attention was found to degrade performance, possibly due to the increase in parameters comlicating the loss space.</p><p>Using an X-Vector (i.e., using the mean and variance of the feature maps) based architecture, our models naturally handle variable length inputs. This is crucial for AMC, as the length of input signals in the wild can vary significantly. By using the X-Vector approach, we are able to compare the performance of our models with different signal burst lengths. In return, we can better understand the limitations of our models by observing how performance degrades as the burst length deceases. To ensure the graph is not too cluttered while also providing analysis for different modulation schemes, we group the 24 modulation schemes from the RadioML 2018.01A dataset into 4 groups. This grouping can be seen below:</p><ul><li><strong>Amplitude</strong>: OOK, 4ASK, 8ASK, AM-SSB-SC, AM-SSB-WC, AM-DSB-WC, AM-DSB-SC</li><li><strong>Phase</strong>: BPSK, QPSK, 8PSK, 16PSK, 32PSK, OQPSK</li><li><strong>Amplitude and Phase</strong>: 16APSK, 32APSK, 64APSK, 128APSK, 16QAM, 32QAM, 64QAM, 128QAM, 256QAM</li><li><strong>Frequency</strong>: FM, GMSK</li></ul><p>Using our best performing model, 1110, we compare the performance of our model with different burst lengths below:</p><figure><img src="'+m+'" alt="AMC performance under various burst lengths for model 1110." tabindex="0" loading="lazy"><figcaption>AMC performance under various burst lengths for model 1110.</figcaption></figure>',16),T={class:"MathJax",jax:"SVG",style:{position:"relative"}},A={style:{"vertical-align":"-0.489ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.364ex",height:"1.489ex",role:"img",focusable:"false",viewBox:"0 -442 603 658","aria-hidden":"true"},M=t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mi"},[t("path",{"data-c":"1D707",d:"M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"})])])],-1),Q=[M],x=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"μ")])],-1),k={class:"MathJax",jax:"SVG",style:{position:"relative"}},S={style:{"vertical-align":"-0.489ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.364ex",height:"1.489ex",role:"img",focusable:"false",viewBox:"0 -442 603 658","aria-hidden":"true"},C=t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mi"},[t("path",{"data-c":"1D707",d:"M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"})])])],-1),P=[C],L=t("mjx-assistive-mml",{unselectable:"on",display:"inline"},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mi",null,"μ")])],-1),D=n('<h2 id="takeaways" tabindex="-1"><a class="header-anchor" href="#takeaways" aria-hidden="true">#</a> Takeaways</h2><p>Factors that increase the receptive field of convolutional layers (e.g., kernel size, dilation rate, SE blocks) are the most influential in improving AMC performance. This is likely due to the fact that AMC is a time series classification task, and the receptive field of the network determines the temporal extent of the features that the network can learn. This work provides a comprehensive analysis of the impacts of various deep learning architectures on AMC performance, and provides guidance for future research in this area.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><hr class="footnotes-sep">',4),N={class:"footnotes"},E={class:"footnotes-list"},B={id:"footnote1",class:"footnote-item"},I={href:"https://ieeexplore.ieee.org/abstract/document/9443358",target:"_blank",rel:"noopener noreferrer"},K=t("a",{href:"#footnote-ref1",class:"footnote-backref"},"↩︎",-1),V={id:"footnote2",class:"footnote-item"},R={href:"https://ieeexplore.ieee.org/abstract/document/9723370",target:"_blank",rel:"noopener noreferrer"},j=t("div",{class:"hint-container important"},[t("p",{class:"hint-container-title"},"See Also"),t("p",null,[e("I have also written a short article on this site, "),t("a",{href:"/publications/snr_boosted_amc"},"SNR-Boosted Automatic Modulation Classification"),e(" that provides a 50,000 foot view of this work.")])],-1),z=t("a",{href:"#footnote-ref2",class:"footnote-backref"},"↩︎",-1),q={id:"footnote3",class:"footnote-item"},H={href:"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html",target:"_blank",rel:"noopener noreferrer"},F=t("a",{href:"#footnote-ref3",class:"footnote-backref"},"↩︎",-1);function W(O,X){const c=a("Share"),l=a("Badge"),s=a("SiteInfo"),h=a("PDF"),i=a("ExternalLinkIcon");return d(),r("div",null,[o(c,{colorful:"",services:["email","facebook","linkedin","reddit","twitter"]}),g,t("p",null,[v,e(": "),o(l,{text:"Published",type:"tip",vertical:"middle"})]),t("div",null,[o(s,{name:"Paper GitHub",url:"https://github.com/caharper/Automatic-Modulation-Classification-with-Deep-Neural-Networks",repo:"https://github.com/caharper/Automatic-Modulation-Classification-with-Deep-Neural-Networks",preview:"/assets/publications/amc_deep_neural_nets/amc_git.png"}),o(s,{name:"Paper Link",desc:"MDPI Electronics Paper",url:"https://www.mdpi.com/2079-9292/12/18/3962",preview:"/assets/publications/amc_deep_neural_nets/amc_electronics.png"})]),_,t("details",b,[w,o(h,{url:"/assets/publications/amc_deep_neural_nets/amc_with_deep_nns.pdf",zoom:"100"})]),y,t("p",null,[e("We find that performance is resilient to changes in burst length down to about 256 "),t("mjx-container",T,[(d(),r("svg",A,Q)),x]),e("s. However, we are still able to achieve above random chance accuracy down to 16 "),t("mjx-container",k,[(d(),r("svg",S,P)),L]),e("s. This is crucial for AMC, as the length of recieved signals is not a guarantee and can vary significantly with various channel conditions.")]),D,t("section",N,[t("ol",E,[t("li",B,[t("p",null,[t("a",I,[e("Enhanced Automatic Modulation Classification using Deep Convolutional Latent Space Pooling"),o(i)]),e(),K])]),t("li",V,[t("p",null,[t("a",R,[e("SNR-Boosted Automatic Modulation Classification"),o(i)])]),j,z]),t("li",q,[t("p",null,[t("a",H,[e("Squeeze-and-excitation networks"),o(i)]),e(),F])])])])])}const J=u(f,[["render",W],["__file","amc_deep_neural_nets.html.vue"]]);export{J as default};
