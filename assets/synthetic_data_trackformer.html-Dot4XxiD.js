import{_ as l,r as o,o as h,c as m,b as a,f as e,d as t,e as n}from"./app-Dw2r7XDh.js";const p="/assets/publications/synthetic_data_trackformer/trackformer_demo.gif",u="/assets/LightingComp-C91A01OR.png",f="/assets/CameraMovementGraph-D1XW-4R1.png",g={},b=e("figure",null,[e("img",{src:p,alt:"TrackFormer demo",style:{width:"100%"}}),e("figcaption",null,"TrackFormer running on synthetically generated data.")],-1),v={class:"hint-container info"},_=e("p",{class:"hint-container-title"},"TL;DR",-1),k={href:"https://arxiv.org/abs/2101.02702",target:"_blank",rel:"noopener noreferrer"},y=n('<h2 id="publication-information" tabindex="-1"><a class="header-anchor" href="#publication-information" aria-hidden="true">#</a> Publication Information</h2><p><code>Title</code>: Impacts of Synthetically Generated Data on Trackformer-based Multi-Object Tracking</p><p><code>Authors</code>: Matthew Lee, Clayton Harper, William Flinchbaugh, Eric C. Larson, and Mitchell A. Thornton</p><p><code>Venue</code>: 2024 IEEE Applied Imagery Pattern Recognition (AIPR)</p><p><code>Publication Date</code>: September 2023</p>',5),w=e("code",null,"Status",-1),T={style:{display:"flex","justify-content":"center"}},j=n(`<p><code>BibTeX Citation</code>:</p><div class="language-bibtex line-numbers-mode" data-ext="bibtex"><pre class="language-bibtex"><code>@inproceedings{lee2023impacts,
  title={Impacts of Synthetically Generated Data on Trackformer-based Multi-Object Tracking},
  author={Lee, Matthew and Harper, Clayton and Flinchbaugh, William and Larson, Eric C and Thornton, Mitchell A},
  booktitle={2023 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)},
  pages={1--7},
  year={2023},
  organization={IEEE}
}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,2),x={class:"hint-container details"},F=e("summary",null,"Paper PDF",-1),I=n('<h2 id="paper-contributions" tabindex="-1"><a class="header-anchor" href="#paper-contributions" aria-hidden="true">#</a> Paper Contributions</h2><ol><li>Analysis of the TrackFormer model&#39;s robustness to environmental conditions.</li><li>Identification of performance degradation with smaller object scales and camera movement.</li></ol><h2 id="paper-purpose" tabindex="-1"><a class="header-anchor" href="#paper-purpose" aria-hidden="true">#</a> Paper Purpose</h2><p>Multi-object tracking has become an increasingly important task in computer vision, with applications in autonomous vehicles, surveillance, and robotics. Multi-object tracking differs from object detection (like YOLO) in that it requires maintaining the identity of objects over multiple frames, rather than just detecting objects in a single frame at one point in time.</p><p>Historically, the predominant method of object tracking was tracking-by-detection, where object detection models were used to detect objects in each frame, and then a separate algorithm was used to associate detections across frames. An example of this is DeepSORT, which uses a YOLO object detector and the Hungarian algorithm to associate detections across frames. The problem with tracking-by-detection is that there are two systems, the detector and the tracker, that must be trained separately and then work together. This can lead to suboptimal performance, as the detector and tracker are not jointly optimized and don&#39;t have information flow between them.</p><p>Very recently, the first end-to-end multi-object tracking model called TrackFormer<sup class="footnote-ref"><a href="#footnote1">[1]</a><a class="footnote-anchor" id="footnote-ref1"></a></sup> was proposed. TrackFormer is a transformer-based model that uses self-attention to associate objects across frames. The model is trained end-to-end, meaning that the object detection and tracking are jointly optimized. This has the potential to lead to better performance, as the detector and tracker can share information and be jointly optimized. From a high level, the model accepts an image along with a history object (updated with each frame through self-attention) and outputs the object tracks.</p><p>Although TrackFormer showed state-of-the-art performance on several tracking benchmarks, many benchmarks are based on relatively optimal scenarios. Although some benchmarks contain a significant number of objects and occlusions (when objects overlap with one another), many environmental conditions are kept mostly constant. Specifically, lighting conditions, object scale, and camera movement are often kept constant. In real-world scenarios, these conditions can vary significantly, and it is important to understand how TrackFormer performs under these conditions. The primary purpose of our work was to identify the strengths and weaknesses of the TrackFormer architecture under various conditions to better understand physical limitations and applications of the work.</p><h2 id="methodology" tabindex="-1"><a class="header-anchor" href="#methodology" aria-hidden="true">#</a> Methodology</h2>',8),S={href:"https://www.presagis.com/en/product/stage/",target:"_blank",rel:"noopener noreferrer"},E=n('<figure><img src="'+u+'" alt="Synthetic data under differing lighting conditions" tabindex="0" loading="lazy"><figcaption>Synthetic data under differing lighting conditions</figcaption></figure><p>In this work, we used the baseline TrackFormer architecture and trained a large number of models on the synthetic data under the various conditions. We then evaluated the models on a test set with the same conditions. We found that the model was robust to changes in lighting conditions, but that the performance degraded with smaller object scales and camera movement.</p><p>In particular, we found the model struggled to track objects when the camera was not stationary, even when trained on data with camera movement (see below).</p><figure><img src="'+f+'" alt="TrackFormer performance with various degrees of camera movement" tabindex="0" loading="lazy"><figcaption>TrackFormer performance with various degrees of camera movement</figcaption></figure><h2 id="takeaways" tabindex="-1"><a class="header-anchor" href="#takeaways" aria-hidden="true">#</a> Takeaways</h2><p>TrackFormer is robust to changes in lighting conditions; however, there are likely architectural advancements that could be made to improve performance under smaller object scales and camera movement. We hope this work will motivate future research to enhance the TrackFormer architecture to better handle these conditions and improve the real-world applicability of the architecture.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><hr class="footnotes-sep">',8),P={class:"footnotes"},A={class:"footnotes-list"},D={id:"footnote1",class:"footnote-item"},C={href:"https://arxiv.org/abs/2101.02702",target:"_blank",rel:"noopener noreferrer"},L=e("a",{href:"#footnote-ref1",class:"footnote-backref"},"↩︎",-1);function M(R,W){const r=o("Share"),i=o("ExternalLinkIcon"),s=o("Badge"),c=o("SiteInfo"),d=o("PDF");return h(),m("div",null,[a(r,{colorful:"",services:["email","facebook","linkedin","reddit","twitter"]}),b,e("div",v,[_,e("p",null,[t("We explore various environmental conditions on the "),e("a",k,[t("TrackFormer"),a(i)]),t(" multi-object tracking model. We find that the model is robust to changes in environmental conditions, such as lighting, subject scale, and camera movement. To investigate a wide range of these conditions, we generate synthetic data with the Presagis STAGE software suite. Ultimately, we find that the model performance degrades with smaller object scales and camera movement, which could motivate ehancements to the TrackFormer architecture.")])]),y,e("p",null,[w,t(": "),a(s,{text:"Published",type:"tip",vertical:"middle"})]),e("div",T,[a(c,{name:"Paper Link",desc:"Impacts of Synthetically Generated Data on Trackformer-based Multi-Object Tracking",url:"https://ieeexplore.ieee.org/abstract/document/10440703",preview:"/assets/publications/synthetic_data_trackformer/aipr_paper.png"})]),j,e("details",x,[F,a(d,{url:"/assets/publications/synthetic_data_trackformer/Synthetic_Data_Trackformer_AIPR.pdf"})]),I,e("p",null,[t("To investigate the performance under various conditions, we needed a large enough dataset to train and evaluate the model. Collecting a large dataset with varying environmental conditions is difficult and time-consuming. To combat this, we used the "),e("a",S,[t("Presagis STAGE software suite"),a(i)]),t(" to generate synthetic data. STAGE is a 3D modeling and simulation software that allows for the creation of complex environments with varying lighting conditions, object scales, and camera movement. We used STAGE to generate a large dataset with varying environmental conditions to train and evaluate the TrackFormer model. Using a simulator allowed us to generate a large dataset with varying environmental conditions, but it also enabled us to ensure the integrity of the data and to control the conditions to a granular level. This is important for evaluating the model's performance.")]),E,e("section",P,[e("ol",A,[e("li",D,[e("p",null,[e("a",C,[t("https://arxiv.org/abs/2101.02702"),a(i)]),t(),L])])])])])}const G=l(g,[["render",M],["__file","synthetic_data_trackformer.html.vue"]]);export{G as default};
