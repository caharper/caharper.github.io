import{_ as c,r as p,o as l,c as u,b as a,f as n,d as s,e}from"./app-Dw2r7XDh.js";const r={},d=e('<div class="hint-container info"><p class="hint-container-title">TL;DR</p><p>Ever experience a <code>Error: OOM ResourceExhaustedError</code> while training a deep learning model? In this blog post, I cover the concept of gradient checkpointing. This technique helps reduce memory consumption during neural network training. However, there aren&#39;t many concrete examples on how to use gradient checkpointing in TensorFlow and Keras. Here, I provide a reproducible code example to help you understand the concept. With this tip, you may be able to train models that previously couldn&#39;t fit in memory!</p><p>In this tutorial (<a href="#a-reproducible-keras-example">jump to code</a>), I show you how to use gradient checkpointing on a Keras model while keeping the training process compatible with built-in Keras callbacks! ðŸ˜±</p></div><div class="hint-container caution"><p class="hint-container-title">Spoiler alert:</p><p><code>tf.recompute_grad</code> does not work with Keras&#39; <code>.fit()</code> method</p></div><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p>Modern neural networks are predominantly trained using the backpropagation algorithm based on gradient descent. At the heart of the widely used deep learning frameworks like TensorFlow, PyTorch, and Flax (JAX), the backpropagation algorithm computes the gradients of the loss function with respect to the model parameters through automatic differentiation, or <em>autodiff</em>. However, as models grow in size and complexity, so do their resource demands. In particular, memory consumption can become a bottleneck or even prohibitively expensive. One great way to combat this issue is through the use of gradient checkpointing which we will cover in this post.</p><h2 id="autodiff-from-50-000-feet" tabindex="-1"><a class="header-anchor" href="#autodiff-from-50-000-feet" aria-hidden="true">#</a> Autodiff From 50,000 Feet</h2><p>Before we dive into gradient checkpointing, let&#39;s take a step back and review the autodiff process. A basic understanding is needed to understand why gradient checkpointing works. The autodiff process is a key component of the backpropagation algorithm. It computes the gradients of the loss function with respect to the model parameters. This is done by applying the chain rule to the computation graph of the model. The computation graph is a directed acyclic graph (DAG) that represents the flow of data through the model. The popular frameworks break down the computational graph into elementary operations that have known gradient computations. The autodiff process computes the gradients by traversing the computation graph in reverse order, applying the chain rule at each node. This process is efficient and accurate, but it requires storing intermediate activations in memory. This can be a problem for large models, especially when training on GPUs with limited memory.</p><div class="hint-container important"><p class="hint-container-title">Storage of Intermediate Activations</p><p>The autodiff process requires storing intermediate activations in memory. This is where the memory bottleneck can occur! This is specific to the backpropagation (training) phase and not the forward pass (inference) phase. Intermediate activations can be discarded throughout the forward pass (once there are no more edges connected to the representation in the DAG), but they are needed for the backward pass to compute the recurrance relation of the gradients.</p></div><h2 id="introducing-gradient-checkpointing" tabindex="-1"><a class="header-anchor" href="#introducing-gradient-checkpointing" aria-hidden="true">#</a> Introducing Gradient Checkpointing</h2><p>To mitigate the memory consumption during the training process, gradient checkpointing can be used. As opposed to storing intermediate activations for the entire forward pass, gradient checkpointing stores only a subset of the activations. When traversing the DAG in the backward pass, the intermediate activations are recomputed as needed. This lowers the memory footprint at the expense of increased computational (and time) demands.</p>',9),k={class:"hint-container tip"},m=n("p",{class:"hint-container-title"},"Visualizing Gradient Checkpointing",-1),v={href:"https://github.com/cybertronai/gradient-checkpointing",target:"_blank",rel:"noopener noreferrer"},h=n("h2",{id:"how-to-implement-gradient-checkpointing",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#how-to-implement-gradient-checkpointing","aria-hidden":"true"},"#"),s(" How to Implement Gradient Checkpointing")],-1),b=n("p",null,[s("In this tutorial, I will focus on how to implement gradient checkpointing in TensorFlow and Keras. However, gradient checkpointing is also available in PyTorch and JAX. For JAX, see "),n("code",null,"jax.checkpoint"),s(" and for PyTorch, see "),n("code",null,"torch.utils.checkpoint"),s(".")],-1),g=n("p",null,"Something I found frustrating was a lack of documentation on how to use gradient checkpointing in TensorFlow--specifically applying it to Keras models. After getting gradient checkpointing to work, I also wanted to keep my model training compatible with built-in Keras callbacks. Once I figured it all out, I wanted to share a reproducible code example to help others. Let's get started!",-1),_=n("code",null,"tf.recompute_grad",-1),f={href:"https://www.tensorflow.org/api_docs/python/tf/recompute_grad",target:"_blank",rel:"noopener noreferrer"},w=e(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

y <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">my_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;running&quot;</span><span class="token punctuation">)</span>
    z <span class="token operator">=</span> x <span class="token operator">*</span> y
    <span class="token keyword">return</span> z


my_function_recompute <span class="token operator">=</span> tf<span class="token punctuation">.</span>recompute_grad<span class="token punctuation">(</span>my_function<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Using the non-recomputed function:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    r <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> my_function<span class="token punctuation">(</span>r<span class="token punctuation">)</span>

grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>r<span class="token punctuation">,</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>the output will be:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>(no output). If we use the recomputed function:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    r <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> my_function_recompute<span class="token punctuation">(</span>r<span class="token punctuation">)</span>

grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>r<span class="token punctuation">,</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>the output will be:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>running
running
running
running
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Notice the intermediate values were recomputed all 4 times!</p><h3 id="gotchas-in-tensorflow-and-keras" tabindex="-1"><a class="header-anchor" href="#gotchas-in-tensorflow-and-keras" aria-hidden="true">#</a> Gotchas in TensorFlow and Keras</h3><p>There is a catch to the example above. There is a comment in the documentation that says:</p>`,12),y=n("code",null,"f",-1),x={href:"https://www.tensorflow.org/api_docs/python/tf/keras",target:"_blank",rel:"noopener noreferrer"},T=n("code",null,"Model",-1),q=n("code",null,"Layer",-1),I=n("code",null,"f.variables",-1),A=n("code",null,"g",-1),K=n("code",null,"f",-1),S=n("code",null,"g.__wrapped__",-1),C=e(`<p>So we have to make a couple of adjustments to make this work with Keras. We have to make the tape persistent and we have to pass a reference to the model/layer parameters. Here is a simple example also from the documentation:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">print_running_and_return</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;running&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x


model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span>print_running_and_return<span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

model_recompute <span class="token operator">=</span> tf<span class="token punctuation">.</span>recompute_grad<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span>persistent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    r <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> model_recompute<span class="token punctuation">(</span>r<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>This outputs:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>running
running
running
running
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,4),F=n("code",null,"model_recompute",-1),G=n("code",null,".fit()",-1),L={href:"https://github.com/keras-team/keras/blob/601488fd4c1468ae7872e132e0f1c9843df54182/keras/engine/training.py#L1149",target:"_blank",rel:"noopener noreferrer"},N=n("code",null,"tf.recompute_grad",-1),E=e(`<p>So where does that leave us? We have to use a custom training loop. No big deal to implement a custom loop, but it should be more clear in the documentation that <code>tf.recompute_grad</code> is not compatible with Keras&#39; <code>.fit()</code> method! Also, when using a custom training loop, it is easy to write the code in a way that is not compatible with Keras&#39; built-in callbacks.</p><h3 id="a-reproducible-keras-example" tabindex="-1"><a class="header-anchor" href="#a-reproducible-keras-example" aria-hidden="true">#</a> A Reproducible Keras Example</h3><p>First, let&#39;s define a simple Keras model:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>
        tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">&quot;relu&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalAveragePooling2D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">&quot;softmax&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now let&#39;s load in a simple dataset. We&#39;ll use MNIST. To make the example more complete, let&#39;s also define a validation set even though MNIST doesn&#39;t directly come with one. Here, we&#39;ll use the last 10% of the training set as the validation set</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds


<span class="token keyword">def</span> <span class="token function">prepare_ds</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_size <span class="token operator">=</span> <span class="token number">16</span>
    ds <span class="token operator">=</span> <span class="token punctuation">(</span>
        ds<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>x<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span>AUTOTUNE<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">return</span> ds


<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span> ds_val<span class="token punctuation">,</span> ds_test<span class="token punctuation">)</span><span class="token punctuation">,</span> ds_info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span>
    <span class="token string">&quot;mnist&quot;</span><span class="token punctuation">,</span>
    split<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;train[:90%]&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;train[90%:]&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># Prepare the datasets</span>
ds_train <span class="token operator">=</span> prepare_ds<span class="token punctuation">(</span>ds_train<span class="token punctuation">)</span>
ds_val <span class="token operator">=</span> prepare_ds<span class="token punctuation">(</span>ds_val<span class="token punctuation">)</span>
ds_test <span class="token operator">=</span> prepare_ds<span class="token punctuation">(</span>ds_test<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Almost time for the cool stuff. Let&#39;s define our optimizer and some Keras callbacks:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">)</span>

callbacks <span class="token operator">=</span> <span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>TerminateOnNaN<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>TensorBoard<span class="token punctuation">(</span>
        log_dir<span class="token operator">=</span><span class="token string">&quot;./model&quot;</span><span class="token punctuation">,</span>
        profile_batch<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>EarlyStopping<span class="token punctuation">(</span>
        monitor<span class="token operator">=</span><span class="token string">&quot;val_loss&quot;</span><span class="token punctuation">,</span> min_delta<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">15</span><span class="token punctuation">,</span> restore_best_weights<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Since we&#39;ll be using a custom loop, we need to specify the metrics we want to track. We also should put our callbacks into a <code>CallbackList</code> object to make life a bit easier:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Define metrics</span>
train_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>Mean<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;train_loss&quot;</span><span class="token punctuation">)</span>
train_accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>CategoricalAccuracy<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;train_acc&quot;</span><span class="token punctuation">)</span>
val_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>Mean<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;val_loss&quot;</span><span class="token punctuation">)</span>
val_accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>CategoricalAccuracy<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;val_acc&quot;</span><span class="token punctuation">)</span>
metrics <span class="token operator">=</span> <span class="token punctuation">[</span>train_loss<span class="token punctuation">,</span> train_accuracy<span class="token punctuation">,</span> val_loss<span class="token punctuation">,</span> val_accuracy<span class="token punctuation">]</span>

<span class="token comment"># Setup callback list</span>
callback_list <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>CallbackList<span class="token punctuation">(</span>
    callbacks<span class="token punctuation">,</span> add_history<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> model<span class="token operator">=</span>model
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now to the cool stuff. First, let&#39;s wrap our model to use gradient checkpointing:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>model_recompute_grad <span class="token operator">=</span> tf<span class="token punctuation">.</span>recompute_grad<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Let&#39;s define all the custom loop code and then discuss it wholistically below.</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> time

<span class="token comment"># Initialize the callbacks</span>
callback_list<span class="token punctuation">.</span>on_train_begin<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Train the model</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>config<span class="token punctuation">.</span>epochs<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

    <span class="token comment"># Start time of epoch</span>
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Reset the metrics at the start of the next epoch</span>
    train_loss<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>
    train_accuracy<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>
    val_loss<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>
    val_accuracy<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>

    callback_list<span class="token punctuation">.</span>on_epoch_begin<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>

    <span class="token comment"># Iterate over batches</span>
    <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>x_batch_train<span class="token punctuation">,</span> y_batch_train<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">)</span><span class="token punctuation">:</span>
        callback_list<span class="token punctuation">.</span>on_train_batch_begin<span class="token punctuation">(</span>step<span class="token punctuation">)</span>

        <span class="token comment"># Forward pass</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span>persistent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
            tape<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>x_batch_train<span class="token punctuation">)</span>
            train_logits <span class="token operator">=</span> model_recompute_grad<span class="token punctuation">(</span>x_batch_train<span class="token punctuation">)</span>
            loss_value <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_batch_train<span class="token punctuation">,</span> train_logits<span class="token punctuation">)</span>

        <span class="token comment"># Backward pass</span>
        grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss_value<span class="token punctuation">,</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>grad<span class="token punctuation">,</span> model<span class="token punctuation">.</span>trainable_weights<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># Update the training metrics</span>
        train_loss<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>loss_value<span class="token punctuation">)</span>
        train_accuracy<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>y_batch_train<span class="token punctuation">,</span> train_logits<span class="token punctuation">)</span>

        <span class="token comment"># Call the training batch end callback</span>
        callback_list<span class="token punctuation">.</span>on_train_batch_end<span class="token punctuation">(</span>step<span class="token punctuation">)</span>

    <span class="token comment"># Iterate over validation batches</span>
    <span class="token keyword">for</span> x_batch_val<span class="token punctuation">,</span> y_batch_val <span class="token keyword">in</span> val_ds<span class="token punctuation">:</span>
        val_logits <span class="token operator">=</span> model<span class="token punctuation">(</span>x_batch_val<span class="token punctuation">,</span> training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        <span class="token comment"># Update the validation metrics</span>
        val_loss<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>y_batch_val<span class="token punctuation">,</span> val_logits<span class="token punctuation">)</span><span class="token punctuation">)</span>
        val_accuracy<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>y_batch_val<span class="token punctuation">,</span> val_logits<span class="token punctuation">)</span>

    <span class="token comment"># Update logs</span>
    logs <span class="token operator">=</span> <span class="token punctuation">{</span>metric<span class="token punctuation">.</span>name<span class="token punctuation">:</span> metric<span class="token punctuation">.</span>result<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> metric <span class="token keyword">in</span> metrics<span class="token punctuation">}</span>

    <span class="token comment"># Call the epoch end callback</span>
    callback_list<span class="token punctuation">.</span>on_epoch_end<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> logs<span class="token punctuation">)</span>

    <span class="token comment"># Get epoch timing information</span>
    epoch_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start_time
    epoch_time <span class="token operator">=</span> time<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">&quot;%H:%M:%S&quot;</span><span class="token punctuation">,</span> time<span class="token punctuation">.</span>gmtime<span class="token punctuation">(</span>epoch_time<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># Update logs at the end of each epoch</span>
    log_str <span class="token operator">=</span> <span class="token string">&quot;, &quot;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span>k<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>v<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> logs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># Print the logs</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Duration: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_time<span class="token punctuation">}</span></span><span class="token string"> - </span><span class="token interpolation"><span class="token punctuation">{</span>log_str<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

<span class="token comment"># Call the training end callback</span>
callback_list<span class="token punctuation">.</span>on_train_end<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Get the training history</span>
history <span class="token operator">=</span> callback_list<span class="token punctuation">.</span>model<span class="token punctuation">.</span>history

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>We now have a custom training loop that uses gradient checkpointing while still being compatible with Keras&#39; built-in callbacks! Let&#39;s discuss the code in more detail.</p>`,15),z=n("code",null,"tape.gradient",-1),M=n("code",null,"f",-1),j={href:"https://www.tensorflow.org/api_docs/python/tf/keras",target:"_blank",rel:"noopener noreferrer"},D=n("code",null,"Model",-1),P=n("code",null,"Layer",-1),H=n("code",null,"f.variables",-1),B=n("code",null,"g",-1),U=n("code",null,"f",-1),W=n("code",null,"g.__wrapped__",-1),O=n("code",null,"CallbackList",-1),V={href:"https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback",target:"_blank",rel:"noopener noreferrer"},J=n("p",null,"That's it! You can now save memory and train significantly larger Keras models through gradient checkpointing! ðŸŽ‰",-1);function R(X,Y){const i=p("Share"),t=p("ExternalLinkIcon"),o=p("Badge");return l(),u("div",null,[a(i,{colorful:"",services:["email","facebook","linkedin","reddit","twitter"]}),d,n("div",k,[m,n("p",null,[s("Although an older repo, "),n("a",v,[s("Saving memory using gradient-checkpointing"),a(t)]),s(" provides a nice visual of the concept.")])]),h,b,g,n("p",null,[s("The main function we will make use of is "),_,s(". More details about the function can be found in the "),n("a",f,[s("TensorFlow documentation"),a(t)]),s(", but I found this was still lacking in practical examples for Keras models. Here is a simple example from the documentation:")]),w,n("blockquote",null,[n("p",null,[s("If "),y,s(" was a "),n("a",x,[s("tf.keras"),a(t)]),s(),T,s(" or "),q,s(" object, methods and attributes such as "),I,s(" are not available on the returned function "),A,s(". Either keep a reference of "),K,s(" , or use "),S,s(" for accessing these variables and methods.")])]),C,n("p",null,[s("Just like we'd expect! So what's the problem? The problem is that we can't use "),F,s(" with Keras' "),G,s(" method. If you dig into the "),n("a",L,[s("source code"),a(t)]),s(" for Keras, you'll find that the tape is not setup to handle the "),N,s(" function.")]),E,n("ul",null,[n("li",null,[a(o,{text:"Gradient Tape",type:"tip",vertical:"middle"}),s(": Notice that the gradient tape is persistent and "),z,s(" is called passing the model's trainable variables. This makes the code compatible with TensorFlow's gradient checkpointing and that comment we saw earlier:"),n("blockquote",null,[n("p",null,[s("If "),M,s(" was a "),n("a",j,[s("tf.keras"),a(t)]),s(),D,s(" or "),P,s(" object, methods and attributes such as "),H,s(" are not available on the returned function "),B,s(". Either keep a reference of "),U,s(" , or use "),W,s(" for accessing these variables and methods.")])])]),n("li",null,[a(o,{text:"Callbacks",type:"tip",vertical:"middle"}),s(": You'll notice that there are a lot of callback list calls. This is because we are using a "),O,s(" object to manage the callbacks, and we want to maintain compatibility with Keras' callbacks API so that we don't have to code everything from scratch (like PyTorch ðŸ‘€)."),n("ul",null,[n("li",null,[s("If you want more information about the calls that were made, checkout the lifecyle methods in the "),n("a",V,[s("Keras Callbacks API"),a(t)]),s(".")])])]),n("li",null,[a(o,{text:"Metrics",type:"tip",vertical:"middle"}),s(": Most of the metrics code is just to update the state of the metrics. This is important so that we have a useful history object. For example, we reset the metric states at the beginning of each epoch to ensure the metrics are up-to-date with the currently trained model. We then update the state of the metrics at the end of each batch so that the statistics can be estimated smoothly.")])]),J])}const Z=c(r,[["render",R],["__file","gradient_checkpointing.html.vue"]]);export{Z as default};
