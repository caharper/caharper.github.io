import{_ as i,r as n,o as r,c as s,b as l,e as h,p as c,g as d,f as a}from"./app-Dw2r7XDh.js";const p="/assets/projects/dct_demo.gif",u={},t=e=>(c("data-v-4ed79c2a"),e=e(),d(),e),f=h('<figure data-v-4ed79c2a><img src="'+p+'" alt="Animation of learned resizing for height and width across training iterations (batches/steps).  Thank you, Finn, for modeling!" tabindex="0" loading="lazy" data-v-4ed79c2a><figcaption data-v-4ed79c2a>Animation of learned resizing for height and width across training iterations (batches/steps). Thank you, Finn, for modeling!</figcaption></figure><h2 id="project-overview-and-why-it-matters" tabindex="-1" data-v-4ed79c2a><a class="header-anchor" href="#project-overview-and-why-it-matters" aria-hidden="true" data-v-4ed79c2a>#</a> Project Overview and Why It Matters</h2><p data-v-4ed79c2a>When designing neural networks, we often need to specify the feature map resolutions (in terms of height and width) as hyperparameters. This is particularly important in computer vision, where the resolution of the feature maps can significantly impact the performance of the network. Too small of a resolution, and we may lose important information. Too large of a resolution, and we may waste computational resources or allow spurious information to influence network activations. The problems with choosing feature map resolutions as a hyperparameter, wether explicitly through resizing or implicitly through pooling layers, are:</p><ol data-v-4ed79c2a><li data-v-4ed79c2a>To find the optimal resolution, we have to perform a large search over a range of resolutions. This can be computationally and/or monetarily expensive and time-consuming. We have to train many networks!</li><li data-v-4ed79c2a>To lower the cost of training many models, we often need subject matter expertise to guide the search. This can be a significant barrier to entry for those without domain knowledge and increase the cost of training performant models.</li><li data-v-4ed79c2a>If we train many models, most of the models will be discarded as they are not optimal. This is wasteful and inefficient.</li><li data-v-4ed79c2a>The whole point of training neural networks is to learn via the data. If we are manually specifying the resolution, we are not learning the optimal resolution from the data. Instead, we are having to manually tune the model. There has to be a better way!</li></ol><h2 id="technical-contributions" tabindex="-1" data-v-4ed79c2a><a class="header-anchor" href="#technical-contributions" aria-hidden="true" data-v-4ed79c2a>#</a> Technical Contributions</h2><p data-v-4ed79c2a>The main hurdle to learning feature map resolution is that you either keep a pixel, or you remove a pixel. You can&#39;t have a fractional pixel. I am using &quot;pixel&quot; loosely here to refer to a single element in the feature map but is more generally an activation. When you can either keep or remove a pixel, you are faced with a threshold function.</p>',6),m=t(()=>a("p",null,"This is a non-differentiable operation, so we can't backpropagate through it. In other words, modern neural networks cannot learn feature map resolution directly. This is why we are currently stuck using hyperparameters to specify the resolution.",-1)),g=t(()=>a("p",null,"I am currently exploring continuous relaxation techniques to enable gradient-flow through the resizing operation. This will allow us to learn the optimal feature map resolution directly from the data. This is particularly exciting because it means we can learn the optimal resolution in a data-dependent manner in a single pass. No need for a large search over a range of resolutions!",-1)),w=t(()=>a("p",null,"In the gif at the top of this page, you can see the learned resizing for height and width across training iterations (batches/steps) using our approach based on the discrete cosine transform (DCT). This example is applied to the input image for a classification task; however, our layers can be applied at any stage in the network (usually in many places in a single network) and are drop-in replacements for traditional resizing layers. The network is learning to resize the feature maps to the optimal resolution for the task at hand.",-1)),y=t(()=>a("p",null,"While our results and methodology are not quite ready to be shared, we are very close, and we are looking to publish our results in the near future.",-1));function v(e,_){const o=n("ECharts");return r(),s("div",null,[f,l(o,{id:"echarts-37",config:"eJxtkM1qwzAQhO9+ikGnBFySXB186Lk9lVxKyUG117WpIhlJDjbF715pkV1TAvpjtDPSfpXRzsP0vjMaJX4yYHweO1fwEfBTTwVEJT19GTuJnNVaelngQ4gcPC+tJdcaVbNwzXE44N0MFuOTDGFI9o5csM8xY3rwyF2qgdIL0fYqP0ktNUBj7E16T7ZAM+iKf7xjz36tAbomiSjLEqftFWDJD1ZDvBD14rxezCDl6J/1+Nj6Rjdzp605nbittPHiKDYcMLG+ZCV0pxxhHMPY0PILRq5KhgRHdXphE6I99UEjHYEH96WNkC0FzC6kEBf0VGO3Ru6hjPn+++I1z+Zz9gt8sood",title:"Threshold%20Function%20Example",type:"js"}),m,g,w,y])}const T=i(u,[["render",v],["__scopeId","data-v-4ed79c2a"],["__file","dct_resizing.html.vue"]]);export{T as default};
