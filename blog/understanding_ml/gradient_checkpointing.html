<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.11" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://caharper.github.io/blog/understanding_ml/gradient_checkpointing.html"><meta property="og:site_name" content="Clayton Harper"><meta property="og:title" content="Gradient Checkpointing"><meta property="og:description" content="TL;DR Ever experience a Error: OOM ResourceExhaustedError while training a deep learning model? In this blog post, I cover the concept of gradient checkpointing. This technique ..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="article:author" content="Clayton Harper"><meta property="article:tag" content="TensorFlow"><meta property="article:tag" content="Memory Reduction"><meta property="article:tag" content="Neural Networks"><meta property="article:published_time" content="2024-02-20T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Gradient Checkpointing","image":[""],"datePublished":"2024-02-20T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"Clayton Harper","url":"https://caharper.github.io/"}]}</script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&display=swap"><title>Gradient Checkpointing | Clayton Harper</title><meta name="description" content="TL;DR Ever experience a Error: OOM ResourceExhaustedError while training a deep learning model? In this blog post, I cover the concept of gradient checkpointing. This technique ...">
    <link rel="preload" href="/assets/style-ByeKlc0x.css" as="style"><link rel="stylesheet" href="/assets/style-ByeKlc0x.css">
    <link rel="modulepreload" href="/assets/app-Dw2r7XDh.js"><link rel="modulepreload" href="/assets/gradient_checkpointing.html-B10NFges.js"><link rel="modulepreload" href="/assets/gradient_checkpointing.html-BPrTbJtS.js">
    <link rel="prefetch" href="/assets/index.html-BiKQbtAz.js" as="script"><link rel="prefetch" href="/assets/index.html-DRKmPfsf.js" as="script"><link rel="prefetch" href="/assets/index.html-DNi6NInB.js" as="script"><link rel="prefetch" href="/assets/index.html-DTcxWAJ8.js" as="script"><link rel="prefetch" href="/assets/cmos_spectroscopy.html-DRmua3lU.js" as="script"><link rel="prefetch" href="/assets/dct_resizing.html-B2rfgoi5.js" as="script"><link rel="prefetch" href="/assets/smart_tfrecord_writer.html-D8HgkfOh.js" as="script"><link rel="prefetch" href="/assets/spectral_convolution.html-Bqu9cpa4.js" as="script"><link rel="prefetch" href="/assets/index.html-CqAFhW5x.js" as="script"><link rel="prefetch" href="/assets/amc_deep_neural_nets.html-D_5Ys6ac.js" as="script"><link rel="prefetch" href="/assets/learnable_moments.html-DMS_412A.js" as="script"><link rel="prefetch" href="/assets/snr_boosted_amc.html-BhBUrmW1.js" as="script"><link rel="prefetch" href="/assets/synthetic_data_trackformer.html-BP56gGGs.js" as="script"><link rel="prefetch" href="/assets/index.html-Dog0wex8.js" as="script"><link rel="prefetch" href="/assets/dct_basics.html-CjnIYg37.js" as="script"><link rel="prefetch" href="/assets/index.html-BeFuYRHw.js" as="script"><link rel="prefetch" href="/assets/endpoint_layers.html-4bCBeaw1.js" as="script"><link rel="prefetch" href="/assets/understanding_receptive_field.html-DLBCabFN.js" as="script"><link rel="prefetch" href="/assets/404.html-C6OQh4RT.js" as="script"><link rel="prefetch" href="/assets/index.html-CutByM1w.js" as="script"><link rel="prefetch" href="/assets/index.html-BE5FuOzL.js" as="script"><link rel="prefetch" href="/assets/index.html-ioLwqewY.js" as="script"><link rel="prefetch" href="/assets/index.html-C1IVGOen.js" as="script"><link rel="prefetch" href="/assets/cmos_spectroscopy.html-o_zhZdqv.js" as="script"><link rel="prefetch" href="/assets/dct_resizing.html-D30ABvvL.js" as="script"><link rel="prefetch" href="/assets/smart_tfrecord_writer.html-Dh1AGEA-.js" as="script"><link rel="prefetch" href="/assets/spectral_convolution.html-C1M8C_6H.js" as="script"><link rel="prefetch" href="/assets/index.html-B_1MIEPT.js" as="script"><link rel="prefetch" href="/assets/amc_deep_neural_nets.html-BGPfbiqw.js" as="script"><link rel="prefetch" href="/assets/learnable_moments.html-CBNkH9BC.js" as="script"><link rel="prefetch" href="/assets/snr_boosted_amc.html-CzP8-5dD.js" as="script"><link rel="prefetch" href="/assets/synthetic_data_trackformer.html-Dot4XxiD.js" as="script"><link rel="prefetch" href="/assets/index.html-C-1XMSZU.js" as="script"><link rel="prefetch" href="/assets/dct_basics.html-UZi_VcVc.js" as="script"><link rel="prefetch" href="/assets/index.html-BoLYbxI_.js" as="script"><link rel="prefetch" href="/assets/endpoint_layers.html-BH3s8lsr.js" as="script"><link rel="prefetch" href="/assets/understanding_receptive_field.html-qwbW-dAQ.js" as="script"><link rel="prefetch" href="/assets/404.html-KuYC986w.js" as="script"><link rel="prefetch" href="/assets/browser-D6eOinvE.js" as="script"><link rel="prefetch" href="/assets/auto-C0MMSKEI.js" as="script"><link rel="prefetch" href="/assets/index-uOBkQLRT.js" as="script"><link rel="prefetch" href="/assets/reveal.esm-9nNZbZvi.js" as="script"><link rel="prefetch" href="/assets/markdown.esm-BG2Xu2Hd.js" as="script"><link rel="prefetch" href="/assets/highlight.esm-B2Y_eiOr.js" as="script"><link rel="prefetch" href="/assets/math.esm-BZ1CfUwa.js" as="script"><link rel="prefetch" href="/assets/search.esm-DuBqnxcF.js" as="script"><link rel="prefetch" href="/assets/notes.esm-Dp2Bpauq.js" as="script"><link rel="prefetch" href="/assets/zoom.esm-Ctj_eavO.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-SzV8tJDW.js" as="script"><link rel="prefetch" href="/assets/SearchResult-BLsZpJNB.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/"><img class="vp-nav-logo" src="/assets/icon/house-solid.svg" alt><!----><span class="vp-site-name hide-in-pad">Clayton Harper</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Projects" class="vp-link nav-link nav-link" href="/projects/"><span class="font-icon icon fa-fw fa-sm fas fa-laptop-code" style=""></span>Projects<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Blog" class="vp-link nav-link active nav-link active" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-pen-nib" style=""></span>Blog<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Publications" class="vp-link nav-link nav-link" href="/publications/"><span class="font-icon icon fa-fw fa-sm fas fa-file-alt" style=""></span>Publications<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="About" class="vp-link nav-link nav-link" href="/about/"><span class="font-icon icon fa-fw fa-sm fas fa-user" style=""></span>About<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/caharper" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><button type="button" class="search-pro-button" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading clickable"><span class="font-icon icon fa-fw fa-sm fas fa-laptop-code" style=""></span><a aria-label="Projects" class="vp-link nav-link vp-sidebar-title nav-link vp-sidebar-title" href="/projects/"><!---->Projects<!----></a><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="CMOS-Based Rotational Spectroscopy" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/projects/cmos_spectroscopy.html"><!---->CMOS-Based Rotational Spectroscopy<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="DCT Resizing in Neural Networks" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/projects/dct_resizing.html"><!---->DCT Resizing in Neural Networks<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="smart-tfrecord-writer" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/projects/smart_tfrecord_writer.html"><!---->smart-tfrecord-writer<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Spectral Convolution in Neural Networks" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/projects/spectral_convolution.html"><!---->Spectral Convolution in Neural Networks<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading clickable active"><span class="font-icon icon fa-fw fa-sm fas fa-pen-nib" style=""></span><a aria-label="Blog" class="vp-link nav-link active vp-sidebar-title nav-link active vp-sidebar-title" href="/blog/"><!---->Blog<!----></a><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Frequency Analysis</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><!----><span class="vp-sidebar-title">Understanding ML</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a aria-label="Endpoint Layers" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/understanding_ml/endpoint_layers.html"><!---->Endpoint Layers<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Gradient Checkpointing" class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active nav-link active vp-sidebar-link vp-sidebar-page active" href="/blog/understanding_ml/gradient_checkpointing.html"><!---->Gradient Checkpointing<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="Introduction" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/understanding_ml/gradient_checkpointing.html#introduction"><!---->Introduction<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Autodiff From 50,000 Feet" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/understanding_ml/gradient_checkpointing.html#autodiff-from-50-000-feet"><!---->Autodiff From 50,000 Feet<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="Introducing Gradient Checkpointing" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/understanding_ml/gradient_checkpointing.html#introducing-gradient-checkpointing"><!---->Introducing Gradient Checkpointing<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="How to Implement Gradient Checkpointing" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/understanding_ml/gradient_checkpointing.html#how-to-implement-gradient-checkpointing"><!---->How to Implement Gradient Checkpointing<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="Gotchas in TensorFlow and Keras" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/understanding_ml/gradient_checkpointing.html#gotchas-in-tensorflow-and-keras"><!---->Gotchas in TensorFlow and Keras<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="A Reproducible Keras Example" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/blog/understanding_ml/gradient_checkpointing.html#a-reproducible-keras-example"><!---->A Reproducible Keras Example<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li></ul><!--]--></li><li><!--[--><a aria-label="Understanding Receptive Field" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/blog/understanding_ml/understanding_receptive_field.html"><!---->Understanding Receptive Field<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-heading clickable"><span class="font-icon icon fa-fw fa-sm fas fa-file-alt" style=""></span><a aria-label="Publications" class="vp-link nav-link vp-sidebar-title nav-link vp-sidebar-title" href="/publications/"><!---->Publications<!----></a><!----></p><ul class="vp-sidebar-links"><li><!--[--><a aria-label="AMC with Deep Neural Networks" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/publications/amc_deep_neural_nets.html"><!---->AMC with Deep Neural Networks<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Impacts of Synthetically Generated Data on Trackformer-based Multi-Object Tracking" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/publications/synthetic_data_trackformer.html"><!---->Impacts of Synthetically Generated Data on Trackformer-based Multi-Object Tracking<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Learnable Statistical Moments Pooling" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/publications/learnable_moments.html"><!---->Learnable Statistical Moments Pooling<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="SNR-Boosted AMC" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/publications/snr_boosted_amc.html"><!---->SNR-Boosted AMC<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Gradient Checkpointing</h1><div class="page-info"><span class="page-author-info" aria-label="AuthorðŸ–Š" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://caharper.github.io/" target="_blank" rel="noopener noreferrer">Clayton Harper</a></span><span property="author" content="Clayton Harper"></span></span><!----><span class="page-date-info" aria-label="Writing DateðŸ“…" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-02-20T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading TimeâŒ›" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 6 min</span><meta property="timeRequired" content="PT6M"></span><span class="page-category-info" aria-label="CategoryðŸŒˆ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category1" role>Blog</span><span class="page-category-item category4" role>ML Tutorials</span><!--]--><meta property="articleSection" content="Blog,ML Tutorials"></span><span class="page-tag-info" aria-label="TagðŸ·" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag5" role>TensorFlow</span><span class="page-tag-item tag0" role>Memory Reduction</span><span class="page-tag-item tag2" role>Neural Networks</span><!--]--><meta property="keywords" content="TensorFlow,Memory Reduction,Neural Networks"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#introduction">Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#autodiff-from-50-000-feet">Autodiff From 50,000 Feet</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#introducing-gradient-checkpointing">Introducing Gradient Checkpointing</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#how-to-implement-gradient-checkpointing">How to Implement Gradient Checkpointing</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#gotchas-in-tensorflow-and-keras">Gotchas in TensorFlow and Keras</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#a-reproducible-keras-example">A Reproducible Keras Example</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><div class="vp-share-buttons" style=""><!--[--><button type="button" class="vp-share-button" aria-label="email" data-balloon-pos="up"><div class="vp-share-icon colorful" style="background:#1384FF;"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path d="M152 177h720c49 0 89 37 90 83L512 494 63 260c0-46 40-83 89-83M62 349v414c0 46 41 84 90 84h720c49 0 90-38 90-84V349L523 572a24 24 0 0 1-22 0z"/></svg></div></button><!----><!--]--><!--[--><button type="button" class="vp-share-button" aria-label="facebook" data-balloon-pos="up"><div class="vp-share-icon colorful" style="background:#3c599b;"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path d="M295 360h93v-91c0-40 1-101 30-139 30-41 72-68 144-68 118 0 168 17 168 17l-24 138s-39-12-75-12-69 13-69 50v105h149l-10 134H562v468H388V494h-93z"/></svg></div></button><!----><!--]--><!--[--><button type="button" class="vp-share-button" aria-label="reddit" data-balloon-pos="up"><div class="vp-share-icon colorful" style="background:#ff4501;"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path d="M678 779c9 9 9 18 0 27-34 37-90 55-166 55s-132-18-166-55c-9-9-9-18 0-27a17 17 0 0 1 12-6c5 0 9 2 13 6 26 29 74 43 141 43s114-14 141-43a17 17 0 0 1 13-6c5 0 9 2 12 6M400 562a82 82 0 0 1 0 112 70 70 0 0 1-53 23c-20 0-38-8-53-23a78 78 0 0 1-22-56c0-22 7-41 22-56a71 71 0 0 1 106 0m352 56c0 22-7 41-22 56a71 71 0 0 1-53 23c-21 0-38-8-53-23a78 78 0 0 1-22-56c0-22 8-40 22-56 15-16 32-23 53-23 20 0 38 7 53 23 15 15 22 34 22 56m210-106c0-29-10-54-29-74a94 94 0 0 0-71-31c-28 0-52 10-72 31-73-53-160-81-260-85l52-250 168 40c0 21 7 40 21 55 15 16 32 23 53 23s38-7 53-23 22-34 22-56-7-41-22-57a71 71 0 0 0-53-23c-30 0-52 15-67 44L572 63c-10-3-17 2-21 14l-57 276c-101 5-187 33-259 86a94 94 0 0 0-73-32c-28 0-51 10-71 31a105 105 0 0 0-29 74 108 108 0 0 0 57 96 241 241 0 0 0-5 49c0 84 39 156 117 216 78 59 172 89 282 89s205-30 283-89c78-60 117-132 117-216 0-19-2-35-6-50a108 108 0 0 0 55-95"/></svg></div></button><!----><!--]--><!--[--><button type="button" class="vp-share-button" aria-label="twitter" data-balloon-pos="up"><div class="vp-share-icon colorful" style="background:#000;"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024"><path d="m587 451 291-339h-69L555 407 354 112H120l305 446-305 354h68l268-310 213 310h235zM214 163h107l488 699H702z"/></svg></div></button><!----><!--]--></div><div class="hint-container info"><p class="hint-container-title">TL;DR</p><p>Ever experience a <code>Error: OOM ResourceExhaustedError</code> while training a deep learning model? In this blog post, I cover the concept of gradient checkpointing. This technique helps reduce memory consumption during neural network training. However, there aren&#39;t many concrete examples on how to use gradient checkpointing in TensorFlow and Keras. Here, I provide a reproducible code example to help you understand the concept. With this tip, you may be able to train models that previously couldn&#39;t fit in memory!</p><p>In this tutorial (<a href="#a-reproducible-keras-example">jump to code</a>), I show you how to use gradient checkpointing on a Keras model while keeping the training process compatible with built-in Keras callbacks! ðŸ˜±</p></div><div class="hint-container caution"><p class="hint-container-title">Spoiler alert:</p><p><code>tf.recompute_grad</code> does not work with Keras&#39; <code>.fit()</code> method</p></div><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p>Modern neural networks are predominantly trained using the backpropagation algorithm based on gradient descent. At the heart of the widely used deep learning frameworks like TensorFlow, PyTorch, and Flax (JAX), the backpropagation algorithm computes the gradients of the loss function with respect to the model parameters through automatic differentiation, or <em>autodiff</em>. However, as models grow in size and complexity, so do their resource demands. In particular, memory consumption can become a bottleneck or even prohibitively expensive. One great way to combat this issue is through the use of gradient checkpointing which we will cover in this post.</p><h2 id="autodiff-from-50-000-feet" tabindex="-1"><a class="header-anchor" href="#autodiff-from-50-000-feet" aria-hidden="true">#</a> Autodiff From 50,000 Feet</h2><p>Before we dive into gradient checkpointing, let&#39;s take a step back and review the autodiff process. A basic understanding is needed to understand why gradient checkpointing works. The autodiff process is a key component of the backpropagation algorithm. It computes the gradients of the loss function with respect to the model parameters. This is done by applying the chain rule to the computation graph of the model. The computation graph is a directed acyclic graph (DAG) that represents the flow of data through the model. The popular frameworks break down the computational graph into elementary operations that have known gradient computations. The autodiff process computes the gradients by traversing the computation graph in reverse order, applying the chain rule at each node. This process is efficient and accurate, but it requires storing intermediate activations in memory. This can be a problem for large models, especially when training on GPUs with limited memory.</p><div class="hint-container important"><p class="hint-container-title">Storage of Intermediate Activations</p><p>The autodiff process requires storing intermediate activations in memory. This is where the memory bottleneck can occur! This is specific to the backpropagation (training) phase and not the forward pass (inference) phase. Intermediate activations can be discarded throughout the forward pass (once there are no more edges connected to the representation in the DAG), but they are needed for the backward pass to compute the recurrance relation of the gradients.</p></div><h2 id="introducing-gradient-checkpointing" tabindex="-1"><a class="header-anchor" href="#introducing-gradient-checkpointing" aria-hidden="true">#</a> Introducing Gradient Checkpointing</h2><p>To mitigate the memory consumption during the training process, gradient checkpointing can be used. As opposed to storing intermediate activations for the entire forward pass, gradient checkpointing stores only a subset of the activations. When traversing the DAG in the backward pass, the intermediate activations are recomputed as needed. This lowers the memory footprint at the expense of increased computational (and time) demands.</p><div class="hint-container tip"><p class="hint-container-title">Visualizing Gradient Checkpointing</p><p>Although an older repo, <a href="https://github.com/cybertronai/gradient-checkpointing" target="_blank" rel="noopener noreferrer">Saving memory using gradient-checkpointing<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> provides a nice visual of the concept.</p></div><h2 id="how-to-implement-gradient-checkpointing" tabindex="-1"><a class="header-anchor" href="#how-to-implement-gradient-checkpointing" aria-hidden="true">#</a> How to Implement Gradient Checkpointing</h2><p>In this tutorial, I will focus on how to implement gradient checkpointing in TensorFlow and Keras. However, gradient checkpointing is also available in PyTorch and JAX. For JAX, see <code>jax.checkpoint</code> and for PyTorch, see <code>torch.utils.checkpoint</code>.</p><p>Something I found frustrating was a lack of documentation on how to use gradient checkpointing in TensorFlow--specifically applying it to Keras models. After getting gradient checkpointing to work, I also wanted to keep my model training compatible with built-in Keras callbacks. Once I figured it all out, I wanted to share a reproducible code example to help others. Let&#39;s get started!</p><p>The main function we will make use of is <code>tf.recompute_grad</code>. More details about the function can be found in the <a href="https://www.tensorflow.org/api_docs/python/tf/recompute_grad" target="_blank" rel="noopener noreferrer">TensorFlow documentation<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>, but I found this was still lacking in practical examples for Keras models. Here is a simple example from the documentation:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

y <span class="token operator">=</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">my_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;running&quot;</span><span class="token punctuation">)</span>
    z <span class="token operator">=</span> x <span class="token operator">*</span> y
    <span class="token keyword">return</span> z


my_function_recompute <span class="token operator">=</span> tf<span class="token punctuation">.</span>recompute_grad<span class="token punctuation">(</span>my_function<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Using the non-recomputed function:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    r <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> my_function<span class="token punctuation">(</span>r<span class="token punctuation">)</span>

grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>r<span class="token punctuation">,</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>the output will be:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>(no output). If we use the recomputed function:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    r <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> my_function_recompute<span class="token punctuation">(</span>r<span class="token punctuation">)</span>

grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>r<span class="token punctuation">,</span> <span class="token punctuation">[</span>y<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>the output will be:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>running
running
running
running
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Notice the intermediate values were recomputed all 4 times!</p><h3 id="gotchas-in-tensorflow-and-keras" tabindex="-1"><a class="header-anchor" href="#gotchas-in-tensorflow-and-keras" aria-hidden="true">#</a> Gotchas in TensorFlow and Keras</h3><p>There is a catch to the example above. There is a comment in the documentation that says:</p><blockquote><p>If <code>f</code> was a <a href="https://www.tensorflow.org/api_docs/python/tf/keras" target="_blank" rel="noopener noreferrer">tf.keras<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <code>Model</code> or <code>Layer</code> object, methods and attributes such as <code>f.variables</code> are not available on the returned function <code>g</code>. Either keep a reference of <code>f</code> , or use <code>g.__wrapped__</code> for accessing these variables and methods.</p></blockquote><p>So we have to make a couple of adjustments to make this work with Keras. We have to make the tape persistent and we have to pass a reference to the model/layer parameters. Here is a simple example also from the documentation:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">print_running_and_return</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tf<span class="token punctuation">.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;running&quot;</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x


model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Lambda<span class="token punctuation">(</span>print_running_and_return<span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

model_recompute <span class="token operator">=</span> tf<span class="token punctuation">.</span>recompute_grad<span class="token punctuation">(</span>model<span class="token punctuation">)</span>

<span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span>persistent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    r <span class="token operator">=</span> tf<span class="token punctuation">.</span>constant<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> model_recompute<span class="token punctuation">(</span>r<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>This outputs:</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>running
running
running
running
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Just like we&#39;d expect! So what&#39;s the problem? The problem is that we can&#39;t use <code>model_recompute</code> with Keras&#39; <code>.fit()</code> method. If you dig into the <a href="https://github.com/keras-team/keras/blob/601488fd4c1468ae7872e132e0f1c9843df54182/keras/engine/training.py#L1149" target="_blank" rel="noopener noreferrer">source code<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> for Keras, you&#39;ll find that the tape is not setup to handle the <code>tf.recompute_grad</code> function.</p><p>So where does that leave us? We have to use a custom training loop. No big deal to implement a custom loop, but it should be more clear in the documentation that <code>tf.recompute_grad</code> is not compatible with Keras&#39; <code>.fit()</code> method! Also, when using a custom training loop, it is easy to write the code in a way that is not compatible with Keras&#39; built-in callbacks.</p><h3 id="a-reproducible-keras-example" tabindex="-1"><a class="header-anchor" href="#a-reproducible-keras-example" aria-hidden="true">#</a> A Reproducible Keras Example</h3><p>First, let&#39;s define a simple Keras model:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>
        tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Conv2D<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">&quot;relu&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>GlobalAveragePooling2D<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">&quot;softmax&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now let&#39;s load in a simple dataset. We&#39;ll use MNIST. To make the example more complete, let&#39;s also define a validation set even though MNIST doesn&#39;t directly come with one. Here, we&#39;ll use the last 10% of the training set as the validation set</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow_datasets <span class="token keyword">as</span> tfds


<span class="token keyword">def</span> <span class="token function">prepare_ds</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_size <span class="token operator">=</span> <span class="token number">16</span>
    ds <span class="token operator">=</span> <span class="token punctuation">(</span>
        ds<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>batch_size <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span>batch<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> <span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>x<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span>prefetch<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span>AUTOTUNE<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">return</span> ds


<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span> ds_val<span class="token punctuation">,</span> ds_test<span class="token punctuation">)</span><span class="token punctuation">,</span> ds_info <span class="token operator">=</span> tfds<span class="token punctuation">.</span>load<span class="token punctuation">(</span>
    <span class="token string">&quot;mnist&quot;</span><span class="token punctuation">,</span>
    split<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;train[:90%]&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;train[90%:]&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    as_supervised<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    with_info<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># Prepare the datasets</span>
ds_train <span class="token operator">=</span> prepare_ds<span class="token punctuation">(</span>ds_train<span class="token punctuation">)</span>
ds_val <span class="token operator">=</span> prepare_ds<span class="token punctuation">(</span>ds_val<span class="token punctuation">)</span>
ds_test <span class="token operator">=</span> prepare_ds<span class="token punctuation">(</span>ds_test<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Almost time for the cool stuff. Let&#39;s define our optimizer and some Keras callbacks:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">)</span>

callbacks <span class="token operator">=</span> <span class="token punctuation">[</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>TerminateOnNaN<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>TensorBoard<span class="token punctuation">(</span>
        log_dir<span class="token operator">=</span><span class="token string">&quot;./model&quot;</span><span class="token punctuation">,</span>
        profile_batch<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
    tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>EarlyStopping<span class="token punctuation">(</span>
        monitor<span class="token operator">=</span><span class="token string">&quot;val_loss&quot;</span><span class="token punctuation">,</span> min_delta<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">15</span><span class="token punctuation">,</span> restore_best_weights<span class="token operator">=</span><span class="token boolean">True</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Since we&#39;ll be using a custom loop, we need to specify the metrics we want to track. We also should put our callbacks into a <code>CallbackList</code> object to make life a bit easier:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Define metrics</span>
train_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>Mean<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;train_loss&quot;</span><span class="token punctuation">)</span>
train_accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>CategoricalAccuracy<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;train_acc&quot;</span><span class="token punctuation">)</span>
val_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>Mean<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;val_loss&quot;</span><span class="token punctuation">)</span>
val_accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>CategoricalAccuracy<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">&quot;val_acc&quot;</span><span class="token punctuation">)</span>
metrics <span class="token operator">=</span> <span class="token punctuation">[</span>train_loss<span class="token punctuation">,</span> train_accuracy<span class="token punctuation">,</span> val_loss<span class="token punctuation">,</span> val_accuracy<span class="token punctuation">]</span>

<span class="token comment"># Setup callback list</span>
callback_list <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>CallbackList<span class="token punctuation">(</span>
    callbacks<span class="token punctuation">,</span> add_history<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> model<span class="token operator">=</span>model
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Now to the cool stuff. First, let&#39;s wrap our model to use gradient checkpointing:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>model_recompute_grad <span class="token operator">=</span> tf<span class="token punctuation">.</span>recompute_grad<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>Let&#39;s define all the custom loop code and then discuss it wholistically below.</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> time

<span class="token comment"># Initialize the callbacks</span>
callback_list<span class="token punctuation">.</span>on_train_begin<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Train the model</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>config<span class="token punctuation">.</span>epochs<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

    <span class="token comment"># Start time of epoch</span>
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Reset the metrics at the start of the next epoch</span>
    train_loss<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>
    train_accuracy<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>
    val_loss<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>
    val_accuracy<span class="token punctuation">.</span>reset_states<span class="token punctuation">(</span><span class="token punctuation">)</span>

    callback_list<span class="token punctuation">.</span>on_epoch_begin<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>

    <span class="token comment"># Iterate over batches</span>
    <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>x_batch_train<span class="token punctuation">,</span> y_batch_train<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">)</span><span class="token punctuation">:</span>
        callback_list<span class="token punctuation">.</span>on_train_batch_begin<span class="token punctuation">(</span>step<span class="token punctuation">)</span>

        <span class="token comment"># Forward pass</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span>persistent<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
            tape<span class="token punctuation">.</span>watch<span class="token punctuation">(</span>x_batch_train<span class="token punctuation">)</span>
            train_logits <span class="token operator">=</span> model_recompute_grad<span class="token punctuation">(</span>x_batch_train<span class="token punctuation">)</span>
            loss_value <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_batch_train<span class="token punctuation">,</span> train_logits<span class="token punctuation">)</span>

        <span class="token comment"># Backward pass</span>
        grad <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss_value<span class="token punctuation">,</span> model<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>grad<span class="token punctuation">,</span> model<span class="token punctuation">.</span>trainable_weights<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># Update the training metrics</span>
        train_loss<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>loss_value<span class="token punctuation">)</span>
        train_accuracy<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>y_batch_train<span class="token punctuation">,</span> train_logits<span class="token punctuation">)</span>

        <span class="token comment"># Call the training batch end callback</span>
        callback_list<span class="token punctuation">.</span>on_train_batch_end<span class="token punctuation">(</span>step<span class="token punctuation">)</span>

    <span class="token comment"># Iterate over validation batches</span>
    <span class="token keyword">for</span> x_batch_val<span class="token punctuation">,</span> y_batch_val <span class="token keyword">in</span> val_ds<span class="token punctuation">:</span>
        val_logits <span class="token operator">=</span> model<span class="token punctuation">(</span>x_batch_val<span class="token punctuation">,</span> training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        <span class="token comment"># Update the validation metrics</span>
        val_loss<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>y_batch_val<span class="token punctuation">,</span> val_logits<span class="token punctuation">)</span><span class="token punctuation">)</span>
        val_accuracy<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>y_batch_val<span class="token punctuation">,</span> val_logits<span class="token punctuation">)</span>

    <span class="token comment"># Update logs</span>
    logs <span class="token operator">=</span> <span class="token punctuation">{</span>metric<span class="token punctuation">.</span>name<span class="token punctuation">:</span> metric<span class="token punctuation">.</span>result<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> metric <span class="token keyword">in</span> metrics<span class="token punctuation">}</span>

    <span class="token comment"># Call the epoch end callback</span>
    callback_list<span class="token punctuation">.</span>on_epoch_end<span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> logs<span class="token punctuation">)</span>

    <span class="token comment"># Get epoch timing information</span>
    epoch_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start_time
    epoch_time <span class="token operator">=</span> time<span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">&quot;%H:%M:%S&quot;</span><span class="token punctuation">,</span> time<span class="token punctuation">.</span>gmtime<span class="token punctuation">(</span>epoch_time<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># Update logs at the end of each epoch</span>
    log_str <span class="token operator">=</span> <span class="token string">&quot;, &quot;</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f&quot;</span><span class="token interpolation"><span class="token punctuation">{</span>k<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>v<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> logs<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># Print the logs</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Duration: </span><span class="token interpolation"><span class="token punctuation">{</span>epoch_time<span class="token punctuation">}</span></span><span class="token string"> - </span><span class="token interpolation"><span class="token punctuation">{</span>log_str<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>

<span class="token comment"># Call the training end callback</span>
callback_list<span class="token punctuation">.</span>on_train_end<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Get the training history</span>
history <span class="token operator">=</span> callback_list<span class="token punctuation">.</span>model<span class="token punctuation">.</span>history

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>We now have a custom training loop that uses gradient checkpointing while still being compatible with Keras&#39; built-in callbacks! Let&#39;s discuss the code in more detail.</p><ul><li><span class="vp-badge tip" style="vertical-align:middle;">Gradient Tape</span>: Notice that the gradient tape is persistent and <code>tape.gradient</code> is called passing the model&#39;s trainable variables. This makes the code compatible with TensorFlow&#39;s gradient checkpointing and that comment we saw earlier:<blockquote><p>If <code>f</code> was a <a href="https://www.tensorflow.org/api_docs/python/tf/keras" target="_blank" rel="noopener noreferrer">tf.keras<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> <code>Model</code> or <code>Layer</code> object, methods and attributes such as <code>f.variables</code> are not available on the returned function <code>g</code>. Either keep a reference of <code>f</code> , or use <code>g.__wrapped__</code> for accessing these variables and methods.</p></blockquote></li><li><span class="vp-badge tip" style="vertical-align:middle;">Callbacks</span>: You&#39;ll notice that there are a lot of callback list calls. This is because we are using a <code>CallbackList</code> object to manage the callbacks, and we want to maintain compatibility with Keras&#39; callbacks API so that we don&#39;t have to code everything from scratch (like PyTorch ðŸ‘€).<ul><li>If you want more information about the calls that were made, checkout the lifecyle methods in the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback" target="_blank" rel="noopener noreferrer">Keras Callbacks API<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</li></ul></li><li><span class="vp-badge tip" style="vertical-align:middle;">Metrics</span>: Most of the metrics code is just to update the state of the metrics. This is important so that we have a useful history object. For example, we reset the metric states at the beginning of each epoch to ensure the metrics are up-to-date with the currently trained model. We then update the state of the metrics at the end of each batch so that the statistics can be estimated smoothly.</li></ul><p>That&#39;s it! You can now save memory and train significantly larger Keras models through gradient checkpointing! ðŸŽ‰</p></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="Endpoint Layers" class="vp-link nav-link prev nav-link prev" href="/blog/understanding_ml/endpoint_layers.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><!---->Endpoint Layers</div></a><a aria-label="Understanding Receptive Field" class="vp-link nav-link next nav-link next" href="/blog/understanding_ml/understanding_receptive_field.html"><div class="hint">Next<span class="arrow end"></span></div><div class="link">Understanding Receptive Field<!----></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><!----><div class="vp-copyright">Copyright Â© 2024 Clayton Harper </div></footer></div><!--]--><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/assets/app-Dw2r7XDh.js" defer></script>
  </body>
</html>
